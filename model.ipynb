{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUiQ5rXsvenh",
        "outputId": "b66447c4-087a-4b8d-80a0-39c69e44b22e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Breakfast: Scrambled eggs with avocado\n",
            "Lunch: Grilled chicken salad with olive oil dressing\n",
            "Dinner: Grilled salmon with steamed broccoli and cauliflower rice\n"
          ]
        }
      ],
      "source": [
        "class NUTRITIONRecommender:\n",
        "    def __init__(self, age, weight, dietary_preferences):\n",
        "        \"\"\"\n",
        "        Initialize the user health data.\n",
        "        :param age: int - The age of the user.\n",
        "        :param weight: float - The weight of the user in kilograms.\n",
        "        :param dietary_preferences: list - A list of dietary preferences (e.g., ['vegetarian', 'low-carb', etc.]).\n",
        "        \"\"\"\n",
        "        self.age = age\n",
        "        self.weight = weight\n",
        "        self.dietary_preferences = dietary_preferences\n",
        "\n",
        "    def suggest_meal_plan(self):\n",
        "        \"\"\"\n",
        "        Suggest meal plan for breakfast, lunch, and dinner based on the user's age, weight, and dietary preferences.\n",
        "        :return: dict - A dictionary containing suggested meals for breakfast, lunch, and dinner.\n",
        "        \"\"\"\n",
        "        breakfast = self.recommend_breakfast()\n",
        "        lunch = self.recommend_lunch()\n",
        "        dinner = self.recommend_dinner()\n",
        "\n",
        "        return {\n",
        "            \"breakfast\": breakfast,\n",
        "            \"lunch\": lunch,\n",
        "            \"dinner\": dinner\n",
        "        }\n",
        "\n",
        "    def recommend_breakfast(self):\n",
        "        \"\"\"\n",
        "        Recommend a breakfast based on user preferences.\n",
        "        :return: str - Recommended breakfast.\n",
        "        \"\"\"\n",
        "        if 'vegetarian' in self.dietary_preferences:\n",
        "            return \"Oatmeal with fruits and nuts\"\n",
        "        elif 'low-carb' in self.dietary_preferences:\n",
        "            return \"Scrambled eggs with avocado\"\n",
        "        else:\n",
        "            return \"Whole grain toast with peanut butter and a banana\"\n",
        "\n",
        "    def recommend_lunch(self):\n",
        "        \"\"\"\n",
        "        Recommend a lunch based on user preferences.\n",
        "        :return: str - Recommended lunch.\n",
        "        \"\"\"\n",
        "        if 'vegetarian' in self.dietary_preferences:\n",
        "            return \"Quinoa salad with chickpeas and veggies\"\n",
        "        elif 'low-carb' in self.dietary_preferences:\n",
        "            return \"Grilled chicken salad with olive oil dressing\"\n",
        "        else:\n",
        "            return \"Turkey sandwich with whole wheat bread and a side salad\"\n",
        "\n",
        "    def recommend_dinner(self):\n",
        "        \"\"\"\n",
        "        Recommend a dinner based on user preferences.\n",
        "        :return: str - Recommended dinner.\n",
        "        \"\"\"\n",
        "        if 'vegetarian' in self.dietary_preferences:\n",
        "            return \"Stir-fried tofu with vegetables and brown rice\"\n",
        "        elif 'low-carb' in self.dietary_preferences:\n",
        "            return \"Grilled salmon with steamed broccoli and cauliflower rice\"\n",
        "        else:\n",
        "            return \"Grilled chicken with quinoa and roasted vegetables\"\n",
        "\n",
        "# Example usage:\n",
        "user = NUTRITIONRecommender(age=30, weight=70, dietary_preferences=['low-carb'])\n",
        "meal_plan = user.suggest_meal_plan()\n",
        "\n",
        "for meal, recommendation in meal_plan.items():\n",
        "    print(f\"{meal.capitalize()}: {recommendation}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Function to generate random dietary preferences\n",
        "def generate_dietary_preferences():\n",
        "    preferences = ['vegetarian', 'vegan', 'low-carb', 'high-protein', 'paleo', 'keto']\n",
        "    return np.random.choice(preferences)\n",
        "\n",
        "# Number of rows and columns\n",
        "num_rows = 1000  # Number of users\n",
        "num_cols = 100   # Number of columns\n",
        "\n",
        "# Creating base columns\n",
        "data = {\n",
        "    'User_ID': np.arange(1, num_rows + 1),\n",
        "    'Age': np.random.randint(18, 65, size=num_rows),\n",
        "    'Weight': np.random.randint(50, 100, size=num_rows),  # in kilograms\n",
        "    'Height': np.random.randint(150, 200, size=num_rows),  # in cm\n",
        "    'Body_Fat_Percentage': np.round(np.random.uniform(10, 35, size=num_rows), 2),\n",
        "    'Activity_Level': np.random.choice(['Sedentary', 'Active', 'Very Active'], size=num_rows),\n",
        "    'Dietary_Preferences': [generate_dietary_preferences() for _ in range(num_rows)],\n",
        "    'Recommended_Calories': np.random.randint(1500, 3000, size=num_rows)\n",
        "}\n",
        "\n",
        "# Add 92 more synthetic columns to make the total 100 columns\n",
        "for i in range(9, num_cols + 1):\n",
        "    column_name = f'Feature_{i}'  # Giving generic names to synthetic columns\n",
        "    data[column_name] = np.random.random(size=num_rows)  # Random float values between 0 and 1\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv('nutrition_data.csv', index=False)\n",
        "\n",
        "# Show the first few rows of the dataset\n",
        "print(df.head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjPEsQcYwTm1",
        "outputId": "07b635ca-10e6-4cf8-de69-cefed553f8bc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   User_ID  Age  Weight  Height  Body_Fat_Percentage Activity_Level  \\\n",
            "0        1   39      63     183                18.67      Sedentary   \n",
            "1        2   26      90     192                26.46         Active   \n",
            "2        3   50      98     178                25.32    Very Active   \n",
            "3        4   32      58     183                11.48         Active   \n",
            "4        5   22      52     164                17.46         Active   \n",
            "\n",
            "  Dietary_Preferences  Recommended_Calories  Feature_9  Feature_10  ...  \\\n",
            "0        high-protein                  1939   0.794280    0.165424  ...   \n",
            "1               vegan                  1912   0.379562    0.648580  ...   \n",
            "2            low-carb                  2598   0.367203    0.648678  ...   \n",
            "3        high-protein                  2156   0.845184    0.534210  ...   \n",
            "4                keto                  1573   0.641516    0.509760  ...   \n",
            "\n",
            "   Feature_91  Feature_92  Feature_93  Feature_94  Feature_95  Feature_96  \\\n",
            "0    0.797099    0.848429    0.235514    0.823780    0.962055    0.872410   \n",
            "1    0.967128    0.483093    0.741912    0.726837    0.556992    0.147364   \n",
            "2    0.974280    0.158949    0.818173    0.966431    0.012258    0.404901   \n",
            "3    0.326642    0.213115    0.794373    0.320126    0.094860    0.208908   \n",
            "4    0.979029    0.465400    0.590271    0.828565    0.145870    0.185146   \n",
            "\n",
            "   Feature_97  Feature_98  Feature_99  Feature_100  \n",
            "0    0.556748    0.288918    0.768941     0.866226  \n",
            "1    0.465183    0.873258    0.720851     0.660121  \n",
            "2    0.159925    0.576113    0.292336     0.282725  \n",
            "3    0.522591    0.102420    0.078178     0.436895  \n",
            "4    0.530741    0.020646    0.567609     0.341845  \n",
            "\n",
            "[5 rows x 100 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Class to store and save the model details along with evaluation metrics\n",
        "class ModelObject:\n",
        "    def _init_(self, model_name, model, params, best_params, evaluation_metrics, version):\n",
        "        self.model_name = model_name\n",
        "        self.model = model\n",
        "        self.params = params\n",
        "        self.best_params = best_params\n",
        "        self.evaluation_metrics = evaluation_metrics\n",
        "        self.version = version\n",
        "\n",
        "    def log_details(self):\n",
        "        log_message = f\"Model: {self.model_name} (Version: {self.version})\\n\"\n",
        "        log_message += f\"Initial Parameters: {self.params}\\n\"\n",
        "        log_message += f\"Best Parameters after tuning: {self.best_params}\\n\"\n",
        "        log_message += f\"Evaluation Metrics: {self.evaluation_metrics}\\n\"\n",
        "        return log_message\n",
        "\n",
        "    def save(self, save_path):\n",
        "        joblib.dump(self, save_path)\n",
        "        print(f\"Model saved at: {save_path}\")\n",
        "\n",
        "# Base Class for Dataset Handling\n",
        "class Dataset:\n",
        "    def _init_(self):\n",
        "        self.data = None\n",
        "        self.target = None\n",
        "        self.X_train = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_test = None\n",
        "\n",
        "    def load_data(self, file_path):\n",
        "        # Load the dataset\n",
        "        self.data = pd.read_csv(file_path)\n",
        "        self.target = self.data['Calories Intake']  # Assuming this is your target column\n",
        "        self.data.drop(columns=['Calories Intake', 'User ID'], inplace=True)  # Drop target and unnecessary columns\n",
        "\n",
        "    def visualize_data(self):\n",
        "        # Visualize data distributions\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.histplot(self.data, kde=True)\n",
        "        plt.title('Distribution of Features')\n",
        "        plt.show()\n",
        "\n",
        "        # Convert categorical columns to numeric for correlation analysis\n",
        "        if self.data.select_dtypes(include=['object']).shape[1] > 0:\n",
        "            # Use one-hot encoding for categorical features\n",
        "            data_numeric = pd.get_dummies(self.data, drop_first=True)\n",
        "        else:\n",
        "            data_numeric = self.data\n",
        "\n",
        "        # Correlation heatmap\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.heatmap(data_numeric.corr(), annot=True, fmt=\".2f\", cmap='coolwarm', linewidths=0.5)\n",
        "        plt.title('Correlation Heatmap')\n",
        "        plt.show()\n",
        "\n",
        "    def preprocess(self):\n",
        "        # Identify categorical columns\n",
        "        categorical_cols = self.data.select_dtypes(include=['object']).columns.tolist()\n",
        "        numerical_cols = self.data.select_dtypes(exclude=['object']).columns.tolist()\n",
        "\n",
        "        # Create a Column Transformer with OneHotEncoder for categorical features\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', 'passthrough', numerical_cols),  # Keep numerical columns unchanged\n",
        "                ('cat', OneHotEncoder(), categorical_cols)  # One-hot encode categorical columns\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Apply the transformations\n",
        "        self.data = preprocessor.fit_transform(self.data)\n",
        "\n",
        "        # Train-test split\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            self.data, self.target, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "# Base Class for Model Selection and Tuning\n",
        "class ModelSelector:\n",
        "    def _init_(self):\n",
        "        self.models = {\n",
        "            'RandomForest': RandomForestRegressor(),\n",
        "        }\n",
        "        self.best_model_object = None\n",
        "        self.version = 1  # Versioning starts at 1\n",
        "\n",
        "    def hyperparameter_tuning(self, model, param_grid, X_train, y_train):\n",
        "        grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        return grid_search.best_estimator_, grid_search.best_params_\n",
        "\n",
        "    def select_model(self, X_train, y_train, X_test, y_test):\n",
        "        # Define parameter grids for each model\n",
        "        param_grids = {\n",
        "            'RandomForest': {'n_estimators': [50, 100, 200], 'max_depth': [5, 10, 15]},\n",
        "        }\n",
        "\n",
        "        best_score = float('inf')\n",
        "        for model_name, model in self.models.items():\n",
        "            print(f\"Tuning {model_name}...\")\n",
        "            tuned_model, best_params = self.hyperparameter_tuning(model, param_grids[model_name], X_train, y_train)\n",
        "\n",
        "            # Evaluate on test data\n",
        "            y_pred = tuned_model.predict(X_test)\n",
        "            mse = mean_squared_error(y_test, y_pred)\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "            mae = mean_absolute_error(y_test, y_pred)\n",
        "            evaluation_metrics = {\n",
        "                \"MSE\": mse,\n",
        "                \"R2 Score\": r2,\n",
        "                \"MAE\": mae\n",
        "            }\n",
        "\n",
        "            print(f\"{model_name} Test MSE: {mse}\")\n",
        "\n",
        "            # Save model object only if it is the best one\n",
        "            if mse < best_score:\n",
        "                best_score = mse\n",
        "                self.best_model_object = ModelObject(\n",
        "                    model_name=model_name,\n",
        "                    model=tuned_model,\n",
        "                    params=param_grids[model_name],\n",
        "                    best_params=best_params,\n",
        "                    evaluation_metrics=evaluation_metrics,\n",
        "                    version=self.version\n",
        "                )\n",
        "\n",
        "        print(f\"Best Model: {self.best_model_object.model_name}\")\n",
        "        return self.best_model_object\n",
        "\n",
        "    def save_best_model(self):\n",
        "        if self.best_model_object:\n",
        "            # Create the model's versioned file name\n",
        "            save_path = f\"{self.best_model_object.model_name}_v{self.version}.pkl\"\n",
        "            self.best_model_object.save(save_path)\n",
        "            self.version += 1  # Increment the version for the next save\n",
        "\n",
        "# Main AutoML Pipeline\n",
        "class AutoMLPipeline:\n",
        "    def _init_(self, dataset_path):\n",
        "        self.dataset = Dataset()\n",
        "        self.model_selector = ModelSelector()\n",
        "        self.dataset_path = dataset_path\n",
        "\n",
        "    def run(self):\n",
        "        # Load and preprocess data\n",
        "        print(\"Loading and Preprocessing Data...\")\n",
        "        self.dataset.load_data(self.dataset_path)\n",
        "\n",
        "        # Perform EDA and visualization\n",
        "        self.dataset.visualize_data()\n",
        "\n",
        "        self.dataset.preprocess()\n",
        "\n",
        "        # Model Selection and Evaluation\n",
        "        print(\"Selecting the best model...\")\n",
        "        best_model = self.model_selector.select_model(\n",
        "            self.dataset.X_train, self.dataset.y_train,\n",
        "            self.dataset.X_test, self.dataset.y_test\n",
        "        )\n",
        "\n",
        "        # Save the best model with versioning\n",
        "        self.model_selector.save_best_model()\n",
        "\n"
      ],
      "metadata": {
        "id": "HuH5UAWyDgiP"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "class AutoMLPipeline:\n",
        "    def __init__(self, dataset_path):\n",
        "        self.dataset_path = dataset_path\n",
        "        # Initialize other necessary attributes here if needed\n",
        "\n",
        "    def run(self):\n",
        "        # Your logic for running the pipeline\n",
        "        print(f\"Running the pipeline on dataset: {self.dataset_path}\")\n",
        "        # Add your model training, evaluation, etc., here\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset_path = r'/content/nutrition_data.csv'\n",
        "    pipeline = AutoMLPipeline(dataset_path)\n",
        "    pipeline.run()\n",
        "\n",
        "    # Load the best model for inspection\n",
        "    model_file_path = 'RandomForest_v1.pkl'  # Change to the latest model file path if needed\n",
        "    loaded_model_object = joblib.load(model_file_path)\n",
        "\n",
        "    # Inspect the contents of the loaded model object\n",
        "    print(f\"Model Name: {loaded_model_object.model_name}\")\n",
        "    print(f\"Version: {loaded_model_object.version}\")\n",
        "    print(f\"Parameters: {loaded_model_object.params}\")\n",
        "    print(f\"Best Parameters: {loaded_model_object.best_params}\")\n",
        "    print(f\"Evaluation Metrics: {loaded_model_object.evaluation_metrics}\")\n",
        "\n",
        "    # To inspect the model itself, use the sklearn model's methods\n",
        "    model = loaded_model_object.model\n",
        "    print(f\"Model: {model}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "84NeVHPBEYs6",
        "outputId": "f7ca207b-d781-4872-d892-d98ca2993c60"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the pipeline on dataset: /content/nutrition_data.csv\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'RandomForest_v1.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-bf91fe47fe49>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Load the best model for inspection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mmodel_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'RandomForest_v1.pkl'\u001b[0m  \u001b[0;31m# Change to the latest model file path if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mloaded_model_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Inspect the contents of the loaded model object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'RandomForest_v1.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install auto-sklearn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5s341rPb5cPp",
        "outputId": "98d355e0-0173-46a3-bb98-c5bf0a8ec895"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting auto-sklearn\n",
            "  Downloading auto-sklearn-0.15.0.tar.gz (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (71.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (4.12.2)\n",
            "Requirement already satisfied: distro in /usr/lib/python3/dist-packages (from auto-sklearn) (1.7.0)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (1.13.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from auto-sklearn) (1.4.2)\n",
            "Collecting scikit-learn<0.25.0,>=0.24.0 (from auto-sklearn)\n",
            "  Downloading scikit-learn-0.24.2.tar.gz (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn pandas numpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1V5M57ah5t5r",
        "outputId": "ed6c18a2-ec6d-470f-eba7-b818f8924c51"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install cython swig\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srZCuj_I528F",
        "outputId": "147d459c-adb5-4f71-ba3d-c2c795d1998e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (3.0.11)\n",
            "Requirement already satisfied: swig in /usr/local/lib/python3.10/dist-packages (4.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install auto-sklearn\n",
        "import autosklearn.classification\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "4koC8zmi6HJZ",
        "outputId": "1f1ca3c8-7bab-42c0-fc17-4b3e21c94e8d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-19-f5828022c6e8>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-f5828022c6e8>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install auto-sklearn\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('nutrition_data.csv')\n",
        "\n",
        "# Preprocess the dataset\n",
        "X = data[['Age', 'Weight']]\n",
        "y = data['Recommended_Calories']\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "auxYdlMu4S62"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhpBNYSf6oRP",
        "outputId": "452dfb43-7070-4d3f-c0e2-fa309f08b6dc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(     Age  Weight\n",
              " 29    20      81\n",
              " 535   60      62\n",
              " 695   33      80\n",
              " 557   29      52\n",
              " 836   32      59\n",
              " ..   ...     ...\n",
              " 106   42      84\n",
              " 270   28      99\n",
              " 860   52      54\n",
              " 435   21      60\n",
              " 102   24      70\n",
              " \n",
              " [800 rows x 2 columns],\n",
              "      Age  Weight\n",
              " 521   43      52\n",
              " 737   34      69\n",
              " 740   27      52\n",
              " 660   33      82\n",
              " 411   35      97\n",
              " ..   ...     ...\n",
              " 408   22      62\n",
              " 332   22      69\n",
              " 208   37      82\n",
              " 613   56      83\n",
              " 78    22      86\n",
              " \n",
              " [200 rows x 2 columns],\n",
              " 29     2436\n",
              " 535    1769\n",
              " 695    2368\n",
              " 557    1576\n",
              " 836    2002\n",
              "        ... \n",
              " 106    1575\n",
              " 270    1732\n",
              " 860    2549\n",
              " 435    2722\n",
              " 102    1524\n",
              " Name: Recommended_Calories, Length: 800, dtype: int64,\n",
              " 521    1949\n",
              " 737    2522\n",
              " 740    2632\n",
              " 660    2775\n",
              " 411    2975\n",
              "        ... \n",
              " 408    2823\n",
              " 332    1610\n",
              " 208    2858\n",
              " 613    2305\n",
              " 78     2612\n",
              " Name: Recommended_Calories, Length: 200, dtype: int64)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from recommendations import get_recommendations\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "ZMaau5EeClws",
        "outputId": "f6598c20-d038-48a0-a73e-a35b38723fa6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'recommendations'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-155691e3f563>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrecommendations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_recommendations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'recommendations'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "from recommendations import get_recommendations\n",
        "from train_model import train\n",
        "\n",
        "class TestFunctions(unittest.TestCase):\n",
        "\n",
        "    def test_get_recommendations(self):\n",
        "        result = get_recommendations(user_id=1)\n",
        "        self.assertIsInstance(result, list)\n",
        "\n",
        "    def test_train_model(self):\n",
        "        model = train(data=\"sample_data\")\n",
        "        self.assertIsNotNone(model)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "T-vXuJ0hx_aI",
        "outputId": "614ca72e-358b-47ef-b44b-4457646b73f1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'recommendations'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-c11fac9995e9>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munittest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrecommendations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_recommendations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrain_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTestFunctions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munittest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTestCase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'recommendations'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}